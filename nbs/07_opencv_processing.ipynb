{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f5d8165",
   "metadata": {},
   "source": [
    "# 7 — Image Processing with OpenCV in ROS2\n",
    "\n",
    "> Use `cv_bridge` to convert ROS images, then apply OpenCV filters, edge detection, and colour tracking.\n",
    "\n",
    "**Goal:** Build a complete image processing pipeline as a ROS2 node: receive camera images, process with OpenCV, and republish the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp opencv_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8caee4b",
   "metadata": {},
   "source": [
    "## 7.1 The Bridge: ROS Images ↔ OpenCV\n",
    "\n",
    "ROS2 uses `sensor_msgs/msg/Image` for images. OpenCV uses **NumPy arrays**. **`cv_bridge`** converts between them:\n",
    "\n",
    "```\n",
    "sensor_msgs/Image  ──cv_bridge──►  numpy.ndarray (OpenCV)\n",
    "  (ROS message)       .imgmsg_to_cv2()    (H × W × C array)\n",
    "\n",
    "numpy.ndarray      ──cv_bridge──►  sensor_msgs/Image\n",
    "  (OpenCV)            .cv2_to_imgmsg()    (ROS message)\n",
    "```\n",
    "\n",
    "### Installation Check\n",
    "\n",
    "```bash\n",
    "# Should already be installed (notebook 01)\n",
    "python3 -c \"from cv_bridge import CvBridge; print('cv_bridge OK')\"\n",
    "python3 -c \"import cv2; print(f'OpenCV {cv2.__version__} OK')\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00677a8",
   "metadata": {},
   "source": [
    "## 7.2 cv_bridge Basics\n",
    "\n",
    "```python\n",
    "from cv_bridge import CvBridge\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "bridge = CvBridge()\n",
    "\n",
    "# ROS Image → OpenCV (numpy)\n",
    "cv_image = bridge.imgmsg_to_cv2(ros_image_msg, desired_encoding='bgr8')\n",
    "# cv_image is now a numpy array of shape (480, 640, 3)\n",
    "\n",
    "# OpenCV (numpy) → ROS Image\n",
    "ros_image_msg = bridge.cv2_to_imgmsg(cv_image, encoding='bgr8')\n",
    "```\n",
    "\n",
    "### Common Encodings\n",
    "\n",
    "| Encoding | Channels | Used for |\n",
    "|----------|----------|----------|\n",
    "| `bgr8` | 3 | Standard OpenCV colour (Blue-Green-Red) |\n",
    "| `rgb8` | 3 | Standard colour (Red-Green-Blue) |\n",
    "| `mono8` | 1 | Grayscale |\n",
    "| `32FC1` | 1 (float) | Depth maps |\n",
    "| `passthrough` | auto | Use whatever the source has |\n",
    "\n",
    "> **Important:** OpenCV uses **BGR** by default, not RGB. When converting, use `desired_encoding='bgr8'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d52a187",
   "metadata": {},
   "source": [
    "## 7.3 The Processing Node Pattern\n",
    "\n",
    "Every image processing node follows this pattern:\n",
    "\n",
    "```\n",
    "Subscribe to input image → Convert → Process → Convert back → Publish\n",
    "```\n",
    "\n",
    "```\n",
    "/camera/image_raw  →  [Your Node]  →  /camera/processed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b9dc3",
   "metadata": {},
   "source": [
    "## 7.4 Example 1: Grayscale Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe17bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "GRAYSCALE_NODE = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Converts camera images to grayscale using OpenCV.\"\"\"\n",
    "\n",
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from rclpy.qos import qos_profile_sensor_data\n",
    "from sensor_msgs.msg import Image\n",
    "from cv_bridge import CvBridge\n",
    "import cv2\n",
    "\n",
    "\n",
    "class GrayscaleNode(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('grayscale_node')\n",
    "\n",
    "        self.bridge = CvBridge()\n",
    "\n",
    "        # Subscribe to raw camera images\n",
    "        self.subscription = self.create_subscription(\n",
    "            Image,\n",
    "            '/camera/image_raw',\n",
    "            self.image_callback,\n",
    "            qos_profile_sensor_data\n",
    "        )\n",
    "\n",
    "        # Publish processed images on a new topic\n",
    "        self.publisher_ = self.create_publisher(\n",
    "            Image,\n",
    "            '/camera/grayscale',\n",
    "            10\n",
    "        )\n",
    "\n",
    "        self.get_logger().info('GrayscaleNode ready')\n",
    "\n",
    "    def image_callback(self, msg: Image):\n",
    "        # 1. Convert ROS Image → OpenCV\n",
    "        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n",
    "\n",
    "        # 2. Process with OpenCV\n",
    "        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # 3. Convert OpenCV → ROS Image\n",
    "        gray_msg = self.bridge.cv2_to_imgmsg(gray, encoding='mono8')\n",
    "        gray_msg.header = msg.header  # preserve timestamp and frame\n",
    "\n",
    "        # 4. Publish\n",
    "        self.publisher_.publish(gray_msg)\n",
    "\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    node = GrayscaleNode()\n",
    "    try:\n",
    "        rclpy.spin(node)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        node.destroy_node()\n",
    "        rclpy.shutdown()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "print(GRAYSCALE_NODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f128f14",
   "metadata": {},
   "source": [
    "### What's Happening\n",
    "\n",
    "1. **Subscribe** to `/camera/image_raw` with sensor QoS\n",
    "2. **Convert** the ROS message to a NumPy array (BGR format)\n",
    "3. **Process** — convert BGR → Grayscale using `cv2.cvtColor`\n",
    "4. **Convert back** to a ROS Image message\n",
    "5. **Copy the header** — this preserves the timestamp and frame_id (important for synchronization)\n",
    "6. **Publish** on `/camera/grayscale`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed2fe1",
   "metadata": {},
   "source": [
    "## 7.5 Example 2: Edge Detection (Canny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc18b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "EDGE_DETECTION_NODE = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Detects edges in camera images using Canny edge detection.\"\"\"\n",
    "\n",
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from rclpy.qos import qos_profile_sensor_data\n",
    "from sensor_msgs.msg import Image\n",
    "from cv_bridge import CvBridge\n",
    "import cv2\n",
    "\n",
    "\n",
    "class EdgeDetectionNode(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('edge_detection_node')\n",
    "\n",
    "        # Declare configurable parameters\n",
    "        self.declare_parameter('low_threshold', 50)\n",
    "        self.declare_parameter('high_threshold', 150)\n",
    "        self.declare_parameter('blur_kernel_size', 5)\n",
    "\n",
    "        self.bridge = CvBridge()\n",
    "\n",
    "        self.subscription = self.create_subscription(\n",
    "            Image, '/camera/image_raw',\n",
    "            self.image_callback, qos_profile_sensor_data\n",
    "        )\n",
    "        self.publisher_ = self.create_publisher(Image, '/camera/edges', 10)\n",
    "\n",
    "        self.get_logger().info('EdgeDetectionNode ready')\n",
    "\n",
    "    def image_callback(self, msg: Image):\n",
    "        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n",
    "\n",
    "        # Read parameters (allows runtime tuning)\n",
    "        low = self.get_parameter('low_threshold').value\n",
    "        high = self.get_parameter('high_threshold').value\n",
    "        ksize = self.get_parameter('blur_kernel_size').value\n",
    "\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Gaussian blur to reduce noise\n",
    "        blurred = cv2.GaussianBlur(gray, (ksize, ksize), 0)\n",
    "\n",
    "        # Canny edge detection\n",
    "        edges = cv2.Canny(blurred, low, high)\n",
    "\n",
    "        # Publish\n",
    "        edge_msg = self.bridge.cv2_to_imgmsg(edges, encoding='mono8')\n",
    "        edge_msg.header = msg.header\n",
    "        self.publisher_.publish(edge_msg)\n",
    "\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    node = EdgeDetectionNode()\n",
    "    try:\n",
    "        rclpy.spin(node)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        node.destroy_node()\n",
    "        rclpy.shutdown()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "print(EDGE_DETECTION_NODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76060e9f",
   "metadata": {},
   "source": [
    "### Tune at Runtime!\n",
    "\n",
    "```bash\n",
    "# While the node is running, adjust thresholds:\n",
    "ros2 param set /edge_detection_node low_threshold 30\n",
    "ros2 param set /edge_detection_node high_threshold 100\n",
    "\n",
    "# See the effect in rqt_image_view on /camera/edges\n",
    "```\n",
    "\n",
    "This is the power of combining ROS2 parameters with OpenCV — **live tuning without restart**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdbd42",
   "metadata": {},
   "source": [
    "## 7.6 Example 3: Colour Object Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a162f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "COLOR_TRACKER_NODE = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Tracks a coloured object (default: red) and draws a bounding circle.\"\"\"\n",
    "\n",
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from rclpy.qos import qos_profile_sensor_data\n",
    "from sensor_msgs.msg import Image\n",
    "from geometry_msgs.msg import Point\n",
    "from cv_bridge import CvBridge\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ColorTrackerNode(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('color_tracker_node')\n",
    "\n",
    "        # HSV range for red colour detection\n",
    "        # Red wraps around in HSV, so we need two ranges\n",
    "        self.declare_parameter('h_low_1', 0)\n",
    "        self.declare_parameter('h_high_1', 10)\n",
    "        self.declare_parameter('h_low_2', 170)\n",
    "        self.declare_parameter('h_high_2', 180)\n",
    "        self.declare_parameter('s_low', 100)\n",
    "        self.declare_parameter('s_high', 255)\n",
    "        self.declare_parameter('v_low', 100)\n",
    "        self.declare_parameter('v_high', 255)\n",
    "        self.declare_parameter('min_area', 500)  # minimum contour area\n",
    "\n",
    "        self.bridge = CvBridge()\n",
    "\n",
    "        self.subscription = self.create_subscription(\n",
    "            Image, '/camera/image_raw',\n",
    "            self.image_callback, qos_profile_sensor_data\n",
    "        )\n",
    "\n",
    "        # Publish annotated image\n",
    "        self.image_pub = self.create_publisher(Image, '/camera/tracked', 10)\n",
    "        # Publish object position\n",
    "        self.position_pub = self.create_publisher(Point, '/tracked_object/position', 10)\n",
    "\n",
    "        self.get_logger().info('ColorTrackerNode ready — tracking red objects')\n",
    "\n",
    "    def image_callback(self, msg: Image):\n",
    "        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n",
    "\n",
    "        # Convert to HSV colour space\n",
    "        hsv = cv2.cvtColor(cv_image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Read parameters\n",
    "        s_low = self.get_parameter('s_low').value\n",
    "        s_high = self.get_parameter('s_high').value\n",
    "        v_low = self.get_parameter('v_low').value\n",
    "        v_high = self.get_parameter('v_high').value\n",
    "\n",
    "        # Create masks for red (two HSV ranges)\n",
    "        lower1 = np.array([self.get_parameter('h_low_1').value, s_low, v_low])\n",
    "        upper1 = np.array([self.get_parameter('h_high_1').value, s_high, v_high])\n",
    "        lower2 = np.array([self.get_parameter('h_low_2').value, s_low, v_low])\n",
    "        upper2 = np.array([self.get_parameter('h_high_2').value, s_high, v_high])\n",
    "\n",
    "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
    "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
    "        mask = mask1 | mask2\n",
    "\n",
    "        # Clean up the mask\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(\n",
    "            mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "\n",
    "        min_area = self.get_parameter('min_area').value\n",
    "\n",
    "        if contours:\n",
    "            # Find the largest contour\n",
    "            largest = max(contours, key=cv2.contourArea)\n",
    "            area = cv2.contourArea(largest)\n",
    "\n",
    "            if area > min_area:\n",
    "                # Get enclosing circle\n",
    "                ((cx, cy), radius) = cv2.minEnclosingCircle(largest)\n",
    "\n",
    "                # Draw on the image\n",
    "                cv2.circle(cv_image, (int(cx), int(cy)), int(radius), (0, 255, 0), 2)\n",
    "                cv2.circle(cv_image, (int(cx), int(cy)), 5, (0, 0, 255), -1)\n",
    "                cv2.putText(\n",
    "                    cv_image,\n",
    "                    f'({int(cx)}, {int(cy)}) area={int(area)}',\n",
    "                    (int(cx) - 50, int(cy) - int(radius) - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2\n",
    "                )\n",
    "\n",
    "                # Publish position\n",
    "                pos = Point()\n",
    "                pos.x = cx\n",
    "                pos.y = cy\n",
    "                pos.z = float(radius)  # use z for radius\n",
    "                self.position_pub.publish(pos)\n",
    "\n",
    "        # Publish annotated image\n",
    "        out_msg = self.bridge.cv2_to_imgmsg(cv_image, encoding='bgr8')\n",
    "        out_msg.header = msg.header\n",
    "        self.image_pub.publish(out_msg)\n",
    "\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    node = ColorTrackerNode()\n",
    "    try:\n",
    "        rclpy.spin(node)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        node.destroy_node()\n",
    "        rclpy.shutdown()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "print(COLOR_TRACKER_NODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e041aa4",
   "metadata": {},
   "source": [
    "### How Colour Tracking Works\n",
    "\n",
    "```\n",
    "BGR Image  →  HSV Conversion  →  Colour Mask  →  Morphology  →  Contours  →  Largest  →  Draw\n",
    "```\n",
    "\n",
    "1. **HSV Conversion** — HSV (Hue, Saturation, Value) is better for colour detection than BGR because *hue* represents the colour regardless of lighting\n",
    "2. **`inRange`** — Creates a binary mask (white = matches colour, black = doesn't)\n",
    "3. **Morphology** — Removes small noise (OPEN = erode+dilate) and fills gaps (CLOSE = dilate+erode)\n",
    "4. **Contours** — Finds the outlines of white regions in the mask\n",
    "5. **Largest contour** — The biggest blob is our tracked object\n",
    "\n",
    "### Tuning HSV Values\n",
    "\n",
    "Use `rqt_reconfigure` or `ros2 param set` to tune in real time:\n",
    "\n",
    "```bash\n",
    "# To track BLUE instead of red:\n",
    "ros2 param set /color_tracker_node h_low_1 100\n",
    "ros2 param set /color_tracker_node h_high_1 130\n",
    "ros2 param set /color_tracker_node h_low_2 100  # same range (blue doesn't wrap)\n",
    "ros2 param set /color_tracker_node h_high_2 130\n",
    "```\n",
    "\n",
    "Common HSV ranges:\n",
    "\n",
    "| Colour | H range |\n",
    "|--------|--------|\n",
    "| Red | 0-10, 170-180 |\n",
    "| Orange | 10-25 |\n",
    "| Yellow | 25-35 |\n",
    "| Green | 35-85 |\n",
    "| Blue | 100-130 |\n",
    "| Purple | 130-170 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929f77c1",
   "metadata": {},
   "source": [
    "## 7.7 Example 4: Multi-Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c0a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "PIPELINE_NODE = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Image pipeline: applies multiple OpenCV operations in sequence.\"\"\"\n",
    "\n",
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from rclpy.qos import qos_profile_sensor_data\n",
    "from sensor_msgs.msg import Image\n",
    "from cv_bridge import CvBridge\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "class ImagePipelineNode(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('image_pipeline_node')\n",
    "\n",
    "        self.declare_parameter('enable_blur', True)\n",
    "        self.declare_parameter('enable_edges', False)\n",
    "        self.declare_parameter('enable_histogram_eq', True)\n",
    "        self.declare_parameter('show_fps', True)\n",
    "\n",
    "        self.bridge = CvBridge()\n",
    "        self.prev_time = time.time()\n",
    "        self.fps = 0.0\n",
    "\n",
    "        self.subscription = self.create_subscription(\n",
    "            Image, '/camera/image_raw',\n",
    "            self.image_callback, qos_profile_sensor_data\n",
    "        )\n",
    "        self.publisher_ = self.create_publisher(Image, '/camera/processed', 10)\n",
    "\n",
    "        self.get_logger().info('ImagePipelineNode ready')\n",
    "\n",
    "    def image_callback(self, msg: Image):\n",
    "        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n",
    "\n",
    "        # Step 1: Optional Gaussian Blur\n",
    "        if self.get_parameter('enable_blur').value:\n",
    "            cv_image = cv2.GaussianBlur(cv_image, (5, 5), 0)\n",
    "\n",
    "        # Step 2: Optional Histogram Equalization\n",
    "        if self.get_parameter('enable_histogram_eq').value:\n",
    "            # Convert to LAB, equalize L channel, convert back\n",
    "            lab = cv2.cvtColor(cv_image, cv2.COLOR_BGR2LAB)\n",
    "            l, a, b = cv2.split(lab)\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "            l = clahe.apply(l)\n",
    "            lab = cv2.merge([l, a, b])\n",
    "            cv_image = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "        # Step 3: Optional Edge Detection\n",
    "        if self.get_parameter('enable_edges').value:\n",
    "            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n",
    "            edges = cv2.Canny(gray, 50, 150)\n",
    "            cv_image = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # FPS overlay\n",
    "        if self.get_parameter('show_fps').value:\n",
    "            now = time.time()\n",
    "            self.fps = 0.9 * self.fps + 0.1 * (1.0 / max(now - self.prev_time, 1e-6))\n",
    "            self.prev_time = now\n",
    "            cv2.putText(\n",
    "                cv_image, f'FPS: {self.fps:.1f}',\n",
    "                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2\n",
    "            )\n",
    "\n",
    "        # Publish\n",
    "        out_msg = self.bridge.cv2_to_imgmsg(cv_image, encoding='bgr8')\n",
    "        out_msg.header = msg.header\n",
    "        self.publisher_.publish(out_msg)\n",
    "\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    node = ImagePipelineNode()\n",
    "    try:\n",
    "        rclpy.spin(node)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        node.destroy_node()\n",
    "        rclpy.shutdown()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "print(PIPELINE_NODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914b3a6",
   "metadata": {},
   "source": [
    "### Toggle Stages at Runtime\n",
    "\n",
    "```bash\n",
    "# Enable edge detection\n",
    "ros2 param set /image_pipeline_node enable_edges true\n",
    "\n",
    "# Disable blur\n",
    "ros2 param set /image_pipeline_node enable_blur false\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ebd7b3",
   "metadata": {},
   "source": [
    "## 7.8 Complete Launch File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b5195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "VISION_LAUNCH = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Launch camera + OpenCV processing pipeline.\"\"\"\n",
    "\n",
    "from launch import LaunchDescription\n",
    "from launch_ros.actions import Node\n",
    "\n",
    "\n",
    "def generate_launch_description():\n",
    "    return LaunchDescription([\n",
    "        # Camera driver\n",
    "        Node(\n",
    "            package='usb_cam',\n",
    "            executable='usb_cam_node_exe',\n",
    "            name='camera',\n",
    "            parameters=[{\n",
    "                'video_device': '/dev/video0',\n",
    "                'image_width': 640,\n",
    "                'image_height': 480,\n",
    "                'framerate': 30.0,\n",
    "            }],\n",
    "            remappings=[\n",
    "                ('image_raw', '/camera/image_raw'),\n",
    "            ]\n",
    "        ),\n",
    "\n",
    "        # Edge detection node\n",
    "        Node(\n",
    "            package='robot_vision',\n",
    "            executable='edge_detection_node',\n",
    "            name='edge_detection_node',\n",
    "            parameters=[{\n",
    "                'low_threshold': 50,\n",
    "                'high_threshold': 150,\n",
    "            }]\n",
    "        ),\n",
    "\n",
    "        # Colour tracker node\n",
    "        Node(\n",
    "            package='robot_vision',\n",
    "            executable='color_tracker_node',\n",
    "            name='color_tracker_node',\n",
    "        ),\n",
    "    ])\n",
    "'''\n",
    "\n",
    "print(VISION_LAUNCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00daf07c",
   "metadata": {},
   "source": [
    "## 7.9 Performance Tips\n",
    "\n",
    "| Tip | Why |\n",
    "|-----|-----|\n",
    "| **Reduce resolution** (320×240 instead of 640×480) | 4× fewer pixels = 4× faster |\n",
    "| **Skip frames** — process every Nth frame | If CV is slower than camera rate |\n",
    "| **Use ROI** (Region of Interest) | Only process a sub-region |\n",
    "| **Use `cv2.UMat`** for GPU-accelerated OpenCV | If you have OpenCL support |\n",
    "| **Publish less** — only annotated results, not intermediate | Reduces topic bandwidth |\n",
    "\n",
    "### Frame Skipping Pattern\n",
    "\n",
    "```python\n",
    "def image_callback(self, msg):\n",
    "    self.frame_count += 1\n",
    "    if self.frame_count % 3 != 0:  # process every 3rd frame\n",
    "        return\n",
    "    # ... heavy processing ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42ac02",
   "metadata": {},
   "source": [
    "## 7.10 OpenCV Quick Reference\n",
    "\n",
    "Common operations you'll use in ROS2 vision nodes:\n",
    "\n",
    "| Operation | Code |\n",
    "|-----------|------|\n",
    "| Resize | `cv2.resize(img, (320, 240))` |\n",
    "| Crop ROI | `roi = img[y1:y2, x1:x2]` |\n",
    "| Colour convert | `cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)` |\n",
    "| Blur | `cv2.GaussianBlur(img, (5,5), 0)` |\n",
    "| Threshold | `cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)` |\n",
    "| Canny edges | `cv2.Canny(gray, 50, 150)` |\n",
    "| Find contours | `cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)` |\n",
    "| Draw circle | `cv2.circle(img, (cx, cy), r, (0,255,0), 2)` |\n",
    "| Draw rectangle | `cv2.rectangle(img, (x1,y1), (x2,y2), (0,255,0), 2)` |\n",
    "| Put text | `cv2.putText(img, 'text', (x,y), font, scale, color, thickness)` |\n",
    "| Morphology | `cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)` |\n",
    "\n",
    "---\n",
    "\n",
    "**Next →** [Notebook 08: Deep Learning in ROS2](08_deep_learning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
