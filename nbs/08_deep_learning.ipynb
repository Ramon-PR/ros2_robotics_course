{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1317a86e",
   "metadata": {},
   "source": [
    "# 8 â€” Deep Learning in ROS2\n",
    "\n",
    "> Run object detection (YOLO / MobileNet) on live camera streams inside ROS2 nodes.\n",
    "\n",
    "**Goal:** Build a ROS2 node that receives camera images, runs a deep learning model, and publishes detection results â€” ready for downstream decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c954cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp deep_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ebee66",
   "metadata": {},
   "source": [
    "## 8.1 Why Deep Learning in Robotics?\n",
    "\n",
    "Traditional CV (notebook 07) works well for **simple, controlled** environments:\n",
    "- Track a red ball on a white floor âœ…\n",
    "- Detect edges of a conveyor belt âœ…\n",
    "\n",
    "But for **complex, real-world** perception you need deep learning:\n",
    "- \"What objects are in front of me?\" â†’ **Object Detection** (YOLO, SSD)\n",
    "- \"Where can I drive?\" â†’ **Semantic Segmentation** (DeepLab, SegFormer)\n",
    "- \"Where is the person's hand?\" â†’ **Pose Estimation** (MediaPipe, OpenPose)\n",
    "- \"What is this thing?\" â†’ **Classification** (ResNet, EfficientNet)\n",
    "\n",
    "### Object Detection in ROS2 â€” The Pattern\n",
    "\n",
    "```\n",
    "/camera/image_raw  â†’  [DL Detector Node]  â†’  /detections (custom msg)\n",
    "                                           â†’  /camera/annotated (Image)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d3006e",
   "metadata": {},
   "source": [
    "## 8.2 Model Choices\n",
    "\n",
    "| Model | Speed | Accuracy | Best For |\n",
    "|-------|-------|----------|----------|\n",
    "| **YOLOv8n** (nano) | âš¡âš¡âš¡ | â­â­ | Real-time on CPU |\n",
    "| **YOLOv8s** (small) | âš¡âš¡ | â­â­â­ | Good balance CPU/GPU |\n",
    "| **YOLOv8m/l/x** | âš¡ | â­â­â­â­ | GPU only, high accuracy |\n",
    "| **MobileNet-SSD** | âš¡âš¡âš¡ | â­â­ | Edge devices (Jetson, Pi) |\n",
    "| **RT-DETR** | âš¡âš¡ | â­â­â­â­ | Transformer-based, GPU |\n",
    "\n",
    "> **We'll use YOLOv8n** â€” it runs at 15-30 FPS on a CPU and detects 80 COCO classes (person, car, dog, bottleâ€¦).\n",
    "\n",
    "### Install Ultralytics (YOLO)\n",
    "\n",
    "```bash\n",
    "pip3 install ultralytics\n",
    "```\n",
    "\n",
    "The first time you use a model, `ultralytics` auto-downloads the weights (~6 MB for yolov8n)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecc367f",
   "metadata": {},
   "source": [
    "## 8.3 Quick YOLO Test (Outside ROS2)\n",
    "\n",
    "Before integrating into ROS2, verify YOLO works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c19241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "YOLO_TEST = '''\n",
    "# Quick test â€” run this in Python to verify ultralytics works\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load YOLOv8 nano (auto-downloads on first run)\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Run inference on a test image (or use a webcam frame)\n",
    "results = model('https://ultralytics.com/images/bus.jpg')\n",
    "\n",
    "# Print detections\n",
    "for result in results:\n",
    "    for box in result.boxes:\n",
    "        cls_id = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "        label = result.names[cls_id]\n",
    "        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "        print(f'{label}: {conf:.2f} at [{x1:.0f}, {y1:.0f}, {x2:.0f}, {y2:.0f}]')\n",
    "\n",
    "# Output:\n",
    "# person: 0.88 at [17, 231, 801, 768]\n",
    "# bus: 0.87 at [10, 130, 810, 720]\n",
    "# ...\n",
    "'''\n",
    "\n",
    "print(YOLO_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243af12f",
   "metadata": {},
   "source": [
    "## 8.4 The YOLO Detection Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18705de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "YOLO_DETECTOR_NODE = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"ROS2 node that runs YOLOv8 object detection on camera images.\"\"\"\n",
    "\n",
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from rclpy.qos import qos_profile_sensor_data\n",
    "from sensor_msgs.msg import Image\n",
    "from std_msgs.msg import String\n",
    "from cv_bridge import CvBridge\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "class YoloDetectorNode(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('yolo_detector_node')\n",
    "\n",
    "        # Parameters\n",
    "        self.declare_parameter('model_path', 'yolov8n.pt')\n",
    "        self.declare_parameter('confidence_threshold', 0.5)\n",
    "        self.declare_parameter('device', 'cpu')        # 'cpu' or 'cuda:0'\n",
    "        self.declare_parameter('input_topic', '/camera/image_raw')\n",
    "        self.declare_parameter('publish_annotated', True)\n",
    "        self.declare_parameter('max_fps', 15.0)        # throttle inference\n",
    "\n",
    "        # Load the model\n",
    "        model_path = self.get_parameter('model_path').value\n",
    "        device = self.get_parameter('device').value\n",
    "        self.get_logger().info(f'Loading model: {model_path} on {device}')\n",
    "\n",
    "        self.model = YOLO(model_path)\n",
    "        # Warm up the model\n",
    "        import numpy as np\n",
    "        dummy = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "        self.model.predict(dummy, device=device, verbose=False)\n",
    "        self.get_logger().info('Model loaded and warmed up')\n",
    "\n",
    "        self.bridge = CvBridge()\n",
    "        self.last_inference_time = 0.0\n",
    "\n",
    "        # Subscribe to camera\n",
    "        input_topic = self.get_parameter('input_topic').value\n",
    "        self.subscription = self.create_subscription(\n",
    "            Image, input_topic,\n",
    "            self.image_callback, qos_profile_sensor_data\n",
    "        )\n",
    "\n",
    "        # Publishers\n",
    "        self.annotated_pub = self.create_publisher(\n",
    "            Image, '/detections/image', 10\n",
    "        )\n",
    "        self.detections_pub = self.create_publisher(\n",
    "            String, '/detections/json', 10\n",
    "        )\n",
    "\n",
    "        self.get_logger().info(\n",
    "            f'YoloDetectorNode ready â€” subscribed to {input_topic}'\n",
    "        )\n",
    "\n",
    "    def image_callback(self, msg: Image):\n",
    "        # Throttle inference to max_fps\n",
    "        now = time.time()\n",
    "        min_interval = 1.0 / self.get_parameter('max_fps').value\n",
    "        if (now - self.last_inference_time) < min_interval:\n",
    "            return\n",
    "        self.last_inference_time = now\n",
    "\n",
    "        # Convert ROS Image â†’ OpenCV\n",
    "        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n",
    "\n",
    "        # Run inference\n",
    "        conf = self.get_parameter('confidence_threshold').value\n",
    "        device = self.get_parameter('device').value\n",
    "        results = self.model.predict(\n",
    "            cv_image,\n",
    "            conf=conf,\n",
    "            device=device,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # Process results\n",
    "        detections = []\n",
    "        result = results[0]\n",
    "\n",
    "        for box in result.boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            confidence = float(box.conf[0])\n",
    "            label = result.names[cls_id]\n",
    "            x1, y1, x2, y2 = [int(v) for v in box.xyxy[0].tolist()]\n",
    "\n",
    "            detections.append({\n",
    "                'label': label,\n",
    "                'confidence': round(confidence, 3),\n",
    "                'bbox': [x1, y1, x2, y2]\n",
    "            })\n",
    "\n",
    "            # Draw on image\n",
    "            color = (0, 255, 0)\n",
    "            cv2.rectangle(cv_image, (x1, y1), (x2, y2), color, 2)\n",
    "            text = f'{label} {confidence:.2f}'\n",
    "            cv2.putText(\n",
    "                cv_image, text, (x1, y1 - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2\n",
    "            )\n",
    "\n",
    "        # Publish detections as JSON\n",
    "        det_msg = String()\n",
    "        det_msg.data = json.dumps({\n",
    "            'timestamp': msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9,\n",
    "            'count': len(detections),\n",
    "            'detections': detections\n",
    "        })\n",
    "        self.detections_pub.publish(det_msg)\n",
    "\n",
    "        # Add FPS text\n",
    "        inference_time = time.time() - now\n",
    "        fps = 1.0 / max(inference_time, 1e-6)\n",
    "        cv2.putText(\n",
    "            cv_image, f'FPS: {fps:.1f} | Objects: {len(detections)}',\n",
    "            (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2\n",
    "        )\n",
    "\n",
    "        # Publish annotated image\n",
    "        if self.get_parameter('publish_annotated').value:\n",
    "            out_msg = self.bridge.cv2_to_imgmsg(cv_image, encoding='bgr8')\n",
    "            out_msg.header = msg.header\n",
    "            self.annotated_pub.publish(out_msg)\n",
    "\n",
    "        if detections:\n",
    "            summary = ', '.join(\n",
    "                f'{d[\"label\"]}({d[\"confidence\"]:.2f})' for d in detections[:5]\n",
    "            )\n",
    "            self.get_logger().info(f'Detected {len(detections)}: {summary}')\n",
    "\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    node = YoloDetectorNode()\n",
    "    try:\n",
    "        rclpy.spin(node)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        node.destroy_node()\n",
    "        rclpy.shutdown()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "print(YOLO_DETECTOR_NODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae04cc",
   "metadata": {},
   "source": [
    "## 8.5 Architecture of the Detection Node\n",
    "\n",
    "```\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "/camera/image_raw â”€â–ºâ”‚    yolo_detector_node        â”‚â”€â–º /detections/image  (annotated)\n",
    "                    â”‚                              â”‚â”€â–º /detections/json   (structured data)\n",
    "                    â”‚  1. cv_bridge â†’ numpy         â”‚\n",
    "                    â”‚  2. YOLO inference            â”‚\n",
    "                    â”‚  3. Draw bounding boxes       â”‚\n",
    "                    â”‚  4. Serialize detections      â”‚\n",
    "                    â”‚  5. Publish                   â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Why Two Output Topics?\n",
    "\n",
    "| Topic | Purpose | Consumer |\n",
    "|-------|---------|----------|\n",
    "| `/detections/image` | Visual debugging in RViz2/rqt | Human |\n",
    "| `/detections/json` | Structured data for downstream nodes | Planner, logger |\n",
    "\n",
    "### Design Decisions Explained\n",
    "\n",
    "1. **FPS Throttling** â€” YOLO can't keep up with 30 FPS on CPU. We skip frames instead of building an ever-growing queue.\n",
    "2. **JSON on String** â€” In production you'd define a custom `Detection.msg`. For learning, JSON on `std_msgs/String` is simpler.\n",
    "3. **Warm-up** â€” The first inference is always slow (model loading, JIT). We do a dummy inference at startup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac076916",
   "metadata": {},
   "source": [
    "## 8.6 Running the Detection Pipeline\n",
    "\n",
    "```bash\n",
    "# Terminal 1: Camera driver\n",
    "ros2 run usb_cam usb_cam_node_exe\n",
    "\n",
    "# Terminal 2: YOLO detector\n",
    "ros2 run robot_vision yolo_detector_node --ros-args \\\n",
    "  -p model_path:=yolov8n.pt \\\n",
    "  -p confidence_threshold:=0.5 \\\n",
    "  -p device:=cpu \\\n",
    "  -p max_fps:=10.0\n",
    "\n",
    "# Terminal 3: View annotated output\n",
    "ros2 run rqt_image_view rqt_image_view\n",
    "# Select /detections/image from the dropdown\n",
    "\n",
    "# Terminal 4: Monitor detections\n",
    "ros2 topic echo /detections/json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f36f0a",
   "metadata": {},
   "source": [
    "## 8.7 Alternative: MobileNet-SSD with OpenCV DNN\n",
    "\n",
    "If you don't want the `ultralytics` dependency, OpenCV has a built-in DNN module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "MOBILENET_NODE = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"ROS2 node with MobileNet-SSD using OpenCV DNN (no extra dependencies).\"\"\"\n",
    "\n",
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from rclpy.qos import qos_profile_sensor_data\n",
    "from sensor_msgs.msg import Image\n",
    "from cv_bridge import CvBridge\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# COCO class names for MobileNet-SSD\n",
    "CLASSES = [\n",
    "    \"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "    \"bottle\", \"bus\", \"car\", \"cat\", \"chair\",\n",
    "    \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\",\n",
    "    \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "]\n",
    "\n",
    "\n",
    "class MobileNetDetectorNode(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('mobilenet_detector_node')\n",
    "\n",
    "        self.declare_parameter('prototxt_path',\n",
    "            'MobileNetSSD_deploy.prototxt')\n",
    "        self.declare_parameter('model_path',\n",
    "            'MobileNetSSD_deploy.caffemodel')\n",
    "        self.declare_parameter('confidence_threshold', 0.5)\n",
    "\n",
    "        prototxt = self.get_parameter('prototxt_path').value\n",
    "        model = self.get_parameter('model_path').value\n",
    "\n",
    "        self.get_logger().info(f'Loading MobileNet-SSD from {model}')\n",
    "        self.net = cv2.dnn.readNetFromCaffe(prototxt, model)\n",
    "        self.get_logger().info('Model loaded')\n",
    "\n",
    "        self.bridge = CvBridge()\n",
    "\n",
    "        self.subscription = self.create_subscription(\n",
    "            Image, '/camera/image_raw',\n",
    "            self.image_callback, qos_profile_sensor_data\n",
    "        )\n",
    "        self.publisher_ = self.create_publisher(\n",
    "            Image, '/detections/image', 10\n",
    "        )\n",
    "\n",
    "    def image_callback(self, msg: Image):\n",
    "        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n",
    "        h, w = cv_image.shape[:2]\n",
    "\n",
    "        # Create a blob (preprocessed input)\n",
    "        blob = cv2.dnn.blobFromImage(\n",
    "            cv2.resize(cv_image, (300, 300)),\n",
    "            0.007843, (300, 300), 127.5\n",
    "        )\n",
    "\n",
    "        # Run forward pass\n",
    "        self.net.setInput(blob)\n",
    "        detections = self.net.forward()\n",
    "\n",
    "        conf_threshold = self.get_parameter('confidence_threshold').value\n",
    "\n",
    "        for i in range(detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            if confidence > conf_threshold:\n",
    "                class_id = int(detections[0, 0, i, 1])\n",
    "                label = CLASSES[class_id] if class_id < len(CLASSES) else '???'\n",
    "\n",
    "                # Scale bounding box to image size\n",
    "                x1 = int(detections[0, 0, i, 3] * w)\n",
    "                y1 = int(detections[0, 0, i, 4] * h)\n",
    "                x2 = int(detections[0, 0, i, 5] * w)\n",
    "                y2 = int(detections[0, 0, i, 6] * h)\n",
    "\n",
    "                cv2.rectangle(cv_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                text = f'{label}: {confidence:.2f}'\n",
    "                cv2.putText(\n",
    "                    cv_image, text, (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2\n",
    "                )\n",
    "\n",
    "        out_msg = self.bridge.cv2_to_imgmsg(cv_image, encoding='bgr8')\n",
    "        out_msg.header = msg.header\n",
    "        self.publisher_.publish(out_msg)\n",
    "\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    node = MobileNetDetectorNode()\n",
    "    try:\n",
    "        rclpy.spin(node)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        node.destroy_node()\n",
    "        rclpy.shutdown()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "print(MOBILENET_NODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca6e697",
   "metadata": {},
   "source": [
    "### YOLO vs MobileNet-SSD\n",
    "\n",
    "| Aspect | YOLOv8 (ultralytics) | MobileNet-SSD (OpenCV DNN) |\n",
    "|--------|---------------------|---------------------------|\n",
    "| Classes | 80 (COCO) | 21 (VOC) |\n",
    "| Install | `pip install ultralytics` | Nothing â€” OpenCV built-in |\n",
    "| Speed (CPU) | ~50ms (nano) | ~30ms |\n",
    "| Accuracy | Higher | Lower |\n",
    "| Model size | 6 MB (nano) | 23 MB |\n",
    "| Best for | General use | Minimal dependencies / edge |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb72fb1",
   "metadata": {},
   "source": [
    "## 8.8 GPU Acceleration\n",
    "\n",
    "For real-time performance (30+ FPS), use a GPU:\n",
    "\n",
    "### NVIDIA GPU with CUDA\n",
    "\n",
    "```bash\n",
    "# Install PyTorch with CUDA (https://pytorch.org/get-started)\n",
    "pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Then run the node with GPU:\n",
    "ros2 run robot_vision yolo_detector_node --ros-args -p device:=cuda:0\n",
    "```\n",
    "\n",
    "### NVIDIA Jetson (Edge AI)\n",
    "\n",
    "On Jetson Nano/Xavier/Orin:\n",
    "- Use **TensorRT** for maximum speed\n",
    "- Export YOLO to TensorRT: `yolo export model=yolov8n.pt format=engine`\n",
    "- Then load the `.engine` file instead of `.pt`\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "| Platform | Model | FPS |\n",
    "|----------|-------|-----|\n",
    "| CPU (i7) | YOLOv8n | ~15-20 |\n",
    "| GTX 1060 | YOLOv8n | ~80-100 |\n",
    "| RTX 3080 | YOLOv8n | ~200+ |\n",
    "| Jetson Nano | YOLOv8n (TensorRT) | ~15-20 |\n",
    "| Jetson Orin | YOLOv8n (TensorRT) | ~100+ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e93ca4",
   "metadata": {},
   "source": [
    "## 8.9 Custom Detection Messages (Production Approach)\n",
    "\n",
    "For production, define a proper message type instead of JSON:\n",
    "\n",
    "```bash\n",
    "# Create a messages package\n",
    "cd ~/ros2_ws/src\n",
    "ros2 pkg create --build-type ament_cmake robot_vision_msgs\n",
    "mkdir -p robot_vision_msgs/msg\n",
    "```\n",
    "\n",
    "```\n",
    "# robot_vision_msgs/msg/Detection.msg\n",
    "string label\n",
    "float32 confidence\n",
    "int32 x1\n",
    "int32 y1\n",
    "int32 x2\n",
    "int32 y2\n",
    "```\n",
    "\n",
    "```\n",
    "# robot_vision_msgs/msg/DetectionArray.msg\n",
    "std_msgs/Header header\n",
    "Detection[] detections\n",
    "```\n",
    "\n",
    "Custom messages are:\n",
    "- **Type-safe** â€” catches errors at compile time\n",
    "- **Efficient** â€” binary serialization, not JSON text\n",
    "- **Discoverable** â€” `ros2 interface show` works on them\n",
    "- **Standard** â€” other ROS2 tools understand them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b49bbb",
   "metadata": {},
   "source": [
    "## 8.10 Full Pipeline Launch File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b0649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "DL_PIPELINE_LAUNCH = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Launch full detection pipeline: camera â†’ YOLO â†’ visualization.\"\"\"\n",
    "\n",
    "from launch import LaunchDescription\n",
    "from launch.actions import DeclareLaunchArgument\n",
    "from launch.substitutions import LaunchConfiguration\n",
    "from launch_ros.actions import Node\n",
    "\n",
    "\n",
    "def generate_launch_description():\n",
    "    return LaunchDescription([\n",
    "        # Arguments\n",
    "        DeclareLaunchArgument('model', default_value='yolov8n.pt'),\n",
    "        DeclareLaunchArgument('device', default_value='cpu'),\n",
    "        DeclareLaunchArgument('confidence', default_value='0.5'),\n",
    "        DeclareLaunchArgument('video_device', default_value='/dev/video0'),\n",
    "\n",
    "        # Camera driver\n",
    "        Node(\n",
    "            package='usb_cam',\n",
    "            executable='usb_cam_node_exe',\n",
    "            name='camera',\n",
    "            parameters=[{\n",
    "                'video_device': LaunchConfiguration('video_device'),\n",
    "                'image_width': 640,\n",
    "                'image_height': 480,\n",
    "                'framerate': 30.0,\n",
    "            }],\n",
    "            remappings=[('image_raw', '/camera/image_raw')]\n",
    "        ),\n",
    "\n",
    "        # YOLO detector\n",
    "        Node(\n",
    "            package='robot_vision',\n",
    "            executable='yolo_detector_node',\n",
    "            name='yolo_detector',\n",
    "            parameters=[{\n",
    "                'model_path': LaunchConfiguration('model'),\n",
    "                'device': LaunchConfiguration('device'),\n",
    "                'confidence_threshold': LaunchConfiguration('confidence'),\n",
    "                'max_fps': 15.0,\n",
    "            }]\n",
    "        ),\n",
    "    ])\n",
    "'''\n",
    "\n",
    "print(DL_PIPELINE_LAUNCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736c6ca4",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "```bash\n",
    "# With defaults (CPU, yolov8n)\n",
    "ros2 launch robot_vision detection_pipeline_launch.py\n",
    "\n",
    "# With GPU and larger model\n",
    "ros2 launch robot_vision detection_pipeline_launch.py \\\n",
    "  model:=yolov8s.pt device:=cuda:0 confidence:=0.3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352d720",
   "metadata": {},
   "source": [
    "## 8.11 Beyond Object Detection\n",
    "\n",
    "The same node pattern works for other DL tasks:\n",
    "\n",
    "| Task | Model | Output |\n",
    "|------|-------|--------|\n",
    "| **Segmentation** | YOLOv8-seg, DeepLabV3 | Pixel-wise class masks |\n",
    "| **Pose Estimation** | YOLOv8-pose, MediaPipe | Keypoint coordinates |\n",
    "| **Depth Estimation** | MiDaS, Depth Anything | Depth map per pixel |\n",
    "| **OCR** | EasyOCR, PaddleOCR | Text strings |\n",
    "| **Face Detection** | MediaPipe, RetinaFace | Face bounding boxes |\n",
    "\n",
    "The architecture is always:\n",
    "\n",
    "```\n",
    "Subscribe Image â†’ Convert â†’ Inference â†’ Publish Results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9317d57",
   "metadata": {},
   "source": [
    "## 8.12 Summary â€” What You've Built\n",
    "\n",
    "Over these 9 notebooks, you've learned:\n",
    "\n",
    "| Notebook | Key Skill |\n",
    "|----------|----------|\n",
    "| 00 | What ROS2 is and why it matters |\n",
    "| 01 | Installing ROS2, tools, and camera packages |\n",
    "| 02 | Nodes, topics, services, actions, QoS â€” the core architecture |\n",
    "| 03 | Workspace/package structure and *why* each file exists |\n",
    "| 04 | Writing Python nodes with timers and parameters |\n",
    "| 05 | Pub/Sub communication, message types, rosbag |\n",
    "| 06 | Camera drivers, image transport, RViz visualization |\n",
    "| 07 | cv_bridge + OpenCV: grayscale, edges, colour tracking |\n",
    "| 08 | Deep learning: YOLO/MobileNet detection on live camera |\n",
    "\n",
    "### Your Complete Pipeline\n",
    "\n",
    "```\n",
    "USB Camera  â†’  usb_cam driver  â†’  /camera/image_raw  â†’  YOLO Node  â†’  /detections/image\n",
    "                                                                    â†’  /detections/json\n",
    "```\n",
    "\n",
    "From here, you can:\n",
    "- Add a **planner node** that reads `/detections/json` and publishes `/cmd_vel`\n",
    "- Integrate with **Nav2** for autonomous navigation\n",
    "- Add **SLAM** for mapping\n",
    "- Deploy to a **Jetson** or **Raspberry Pi**\n",
    "\n",
    "**Happy building!** ðŸ¤–"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
